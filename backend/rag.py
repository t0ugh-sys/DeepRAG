import json
import os
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple

import numpy as np
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from pymilvus import connections, Collection
from backend.ingest import split_text
from backend.config import Settings
from backend.utils.cache import query_cache
from rank_bm25 import BM25Okapi  # type: ignore

try:
    from FlagEmbedding import FlagReranker  # type: ignore
except Exception:  # pragma: no cover
    FlagReranker = None  # type: ignore


@dataclass
class RetrievedChunk:
    text: str
    score: float
    meta: Dict[str, Any]


class VectorStore:
    """
    å‘é‡å­˜å‚¨æŠ½è±¡å±‚ï¼Œæ”¯æŒ Milvus å’?FAISS åŒåç«?
    
    è‡ªåŠ¨å°è¯•è¿æ¥ Milvusï¼Œå¤±è´¥æ—¶å›é€€åˆ°æœ¬åœ?FAISS ç´¢å¼•
    """
    
    def __init__(self, meta_path: str, embedding_model: str, settings: Settings, namespace: str | None = None) -> None:
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚?
        
        Args:
            meta_path: å…ƒæ•°æ®æ–‡ä»¶è·¯å¾?(meta.jsonl)
            embedding_model: å‘é‡åŒ–æ¨¡å‹åç§?
            settings: é…ç½®å¯¹è±¡
            namespace: å‘½åç©ºé—´
        """
        if not os.path.exists(meta_path):
            raise FileNotFoundError("æœªæ‰¾åˆ?meta.jsonlï¼Œè¯·å…ˆè¿è¡?ingest æ„å»ºç´¢å¼•")
        self.namespace = namespace or settings.default_namespace
        self.texts: List[str] = []
        self.metas: List[Dict[str, Any]] = []
        self.meta_path = meta_path
        with open(meta_path, "r", encoding="utf-8") as f:
            for line in f:
                rec = json.loads(line)
                self.texts.append(rec["text"]) 
                self.metas.append(rec["meta"]) 
        self.embedder = SentenceTransformer(embedding_model)
        # BM25 è¯­æ–™ï¼ˆæŒ‰è¯åˆ†ï¼?
        self._bm25_tokenized = [self._tokenize(t) for t in self.texts]
        self._bm25 = BM25Okapi(self._bm25_tokenized) if self._bm25_tokenized else None

        # å°è¯•ä½¿ç”¨ Milvusï¼Œå¦åˆ™å›é€€åˆ?FAISS
        self.collection = None
        self.faiss_index = None
        self.backend = "faiss"
        collection_name = f"{settings.milvus_collection}_{(namespace or settings.default_namespace)}"
        if settings.vector_backend in ("auto", "milvus"):
            try:
                connections.connect(
                    alias="default",
                    host=settings.milvus_host,
                    port=settings.milvus_port,
                    user=settings.milvus_user,
                    password=settings.milvus_password,
                    secure=settings.milvus_secure,
                )
                self.collection = Collection(collection_name)
                self.backend = "milvus"
            except Exception as e:
                # Milvus è¿æ¥å¤±è´¥ï¼Œå›é€€åˆ?FAISS
                import logging
                logging.getLogger("rag").debug(f"Milvus è¿æ¥å¤±è´¥ï¼Œå›é€€åˆ?FAISS: {e}")
                self.collection = None
                self.backend = "faiss"
        if self.collection is None:
            faiss_path = os.path.join(os.path.dirname(meta_path), "faiss.index")
            self.faiss_path = faiss_path
            if not os.path.exists(faiss_path):
                raise FileNotFoundError("æœªæ‰¾åˆ?Milvus é›†åˆä¸”ç¼ºå°?faiss.indexï¼Œè¯·å…ˆè¿è¡?ingest æ„å»ºç´¢å¼•")
            try:
                import faiss  # type: ignore
            except Exception as exc:  # pragma: no cover
                raise RuntimeError("éœ€è¦?FAISS ä»¥è¯»å–æœ¬åœ°å›é€€ç´¢å¼•ï¼Œè¯·ä½¿ç”¨ conda å®‰è£… faiss-cpu: conda install -n rag-env -c conda-forge faiss-cpu") from exc
            self.faiss_index = faiss.read_index(faiss_path)

    def _expand_query(self, query: str) -> str:
        """æŸ¥è¯¢æ‰©å±•ï¼šæå–å…³é”®è¯ï¼Œç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½?""
        try:
            import jieba.analyse
            # æå–å…³é”®è¯ï¼ˆTF-IDFï¼?
            keywords = jieba.analyse.extract_tags(query, topK=5, withWeight=False)
            # å°†å…³é”®è¯ç»„åˆå›åŸæŸ¥è¯¢
            expanded = query + " " + " ".join(keywords)
            return expanded
        except Exception:
            return query
    
    def search(self, query: str, top_k: int = 4) -> List[RetrievedChunk]:
        # æŸ¥è¯¢æ‰©å±•
        expanded_query = self._expand_query(query)
        vec = self.embedder.encode([expanded_query], normalize_embeddings=True)
        vec = np.array(vec).astype("float32")[0]
        results: List[RetrievedChunk] = []
        if self.backend == "milvus" and self.collection is not None:
            search_params = {"metric_type": "IP", "params": {"nprobe": 16}}
            res = self.collection.search(
                data=[vec],
                anns_field="embedding",
                param=search_params,
                limit=top_k,
                output_fields=["path", "chunk_id", "text"],
            )
            hits = res[0]
            for hit in hits:
                meta = {"path": hit.entity.get("path"), "chunk_id": int(hit.entity.get("chunk_id"))}
                results.append(RetrievedChunk(text=hit.entity.get("text"), score=float(hit.distance), meta=meta))
            return _apply_score_threshold(_dedupe_results(results), self.settings.score_threshold)
        # faiss å›é€€
        assert self.faiss_index is not None
        # åŠ¨æ€å¯¼å…?faissï¼Œé¿å…æœªå®‰è£…æ—¶æŠ¥é”?
        import faiss  # type: ignore
        scores, indices = self.faiss_index.search(vec.reshape(1, -1), top_k)
        for score, idx in zip(scores[0], indices[0]):
            if idx == -1:
                continue
            results.append(RetrievedChunk(text=self.texts[idx], score=float(score), meta=self.metas[idx]))

        # å¯é€?BM25 èåˆ
        return self._fuse_with_bm25(query, _dedupe_results(results), top_k)

    # --- è¾…åŠ©ï¼šBM25 + MMR èåˆ ---
    def _tokenize(self, s: str) -> list[str]:
        """ä¸­æ–‡å‹å¥½çš„åˆ†è¯ï¼šç»“åˆ jieba åˆ†è¯å’Œå­—ç¬¦çº§åˆ†è¯"""
        import re
        try:
            import jieba
            # ä½¿ç”¨ jieba è¿›è¡Œä¸­æ–‡åˆ†è¯
            words = list(jieba.cut_for_search(s.lower()))  # æœç´¢å¼•æ“æ¨¡å¼ï¼Œæ›´ç»†ç²’åº?
            # è¿‡æ»¤æ‰ç©ºç™½å’Œå•å­—ç¬¦æ ‡ç‚?
            return [w.strip() for w in words if w.strip() and not re.match(r'^[\W_]+$', w)]
        except ImportError:
            # å¦‚æœæ²¡æœ‰ jiebaï¼Œå›é€€åˆ°ç®€å•åˆ†è¯?
            return [w for w in re.split(r"\W+", s.lower()) if w]

    def _fuse_with_bm25(self, query: str, vec_results: List[RetrievedChunk], top_k: int) -> List[RetrievedChunk]:
        settings = Settings()
        recs = list(vec_results)
        if settings.bm25_enabled and self._bm25 is not None:
            tokens = self._tokenize(query)
            bm25_scores = self._bm25.get_scores(tokens)
            # å½’ä¸€åŒ–åˆ†æ•?
            import math
            def norm(x: float) -> float:
                return 0.0 if math.isnan(x) else float(x)
            bm25_max = max(bm25_scores) if len(bm25_scores) else 1.0
            fused: dict[int, float] = {}
            for i, r in enumerate(recs):
                bm = (bm25_scores[i] / (bm25_max or 1.0)) if i < len(bm25_scores) else 0.0
                fused[i] = settings.vec_weight * r.score + settings.bm25_weight * norm(bm)
                r.score = fused[i]
            # å¾—åˆ†é˜ˆå€¼è¿‡æ»?
            if settings.score_threshold > 0:
                recs = [r for r in recs if r.score >= settings.score_threshold]
            recs = sorted(recs, key=lambda x: x.score, reverse=True)

        # MMR å¤šæ ·æ€§é‡‡æ ?
        if len(recs) > top_k:
            recs = self._mmr(query, recs, top_k, lambda_weight=settings.mmr_lambda)
        return recs

    def _mmr(self, query: str, recs: List[RetrievedChunk], k: int, lambda_weight: float = 0.5) -> List[RetrievedChunk]:
        # ä½¿ç”¨åµŒå…¥ç©ºé—´ç›¸ä¼¼åº¦è¿‘ä¼¼å»å†—ä½™
        query_vec = self.embedder.encode([query], normalize_embeddings=True)
        query_vec = np.array(query_vec).astype("float32")[0]
        cand_vecs = self.embedder.encode([r.text for r in recs], normalize_embeddings=True)
        cand_vecs = np.array(cand_vecs).astype("float32")
        selected: list[int] = []
        remaining = set(range(len(recs)))
        def sim(a: np.ndarray, b: np.ndarray) -> float:
            return float(np.dot(a, b))
        while remaining and len(selected) < k:
            if not selected:
                # å…ˆé€‰ä¸ query æœ€ç›¸ä¼¼
                idx = max(remaining, key=lambda i: sim(query_vec, cand_vecs[i]))
                selected.append(idx)
                remaining.remove(idx)
                continue
            best_idx = None
            best_score = -1e9
            for i in list(remaining):
                relevance = sim(query_vec, cand_vecs[i])
                diversity = max(sim(cand_vecs[i], cand_vecs[j]) for j in selected)
                mmr_score = lambda_weight * relevance - (1 - lambda_weight) * diversity
                if mmr_score > best_score:
                    best_score = mmr_score
                    best_idx = i
            selected.append(best_idx)  # type: ignore[arg-type]
            remaining.remove(best_idx)  # type: ignore[arg-type]
        return [recs[i] for i in selected]

    def add_document(self, path: str, text: str) -> int:
        # æ”¯æŒä¸¤ç§åç«¯çš„åœ¨çº¿æ–°å¢?
        if self.backend == "milvus" and self.collection is not None:
            chunks = split_text(text)
            if not chunks:
                return 0
            embeddings = self.embedder.encode(chunks, normalize_embeddings=True)
            embeddings = np.array(embeddings).astype("float32")
            paths = [path] * len(chunks)
            chunk_ids = list(range(len(chunks)))
            self.collection.insert([paths, chunk_ids, chunks, embeddings])
            self.collection.flush()
            return len(chunks)

        # FAISS æœ¬åœ°æ¨¡å¼ï¼šåŠ¨æ€è¿½åŠ å¹¶å†™å›ç´¢å¼•ä¸?meta
        if self.backend == "faiss" and self.faiss_index is not None:
            chunks = split_text(text)
            if not chunks:
                return 0
            import faiss  # type: ignore
            embeddings = self.embedder.encode(chunks, normalize_embeddings=True)
            vecs = np.array(embeddings).astype("float32")
            self.faiss_index.add(vecs)
            # å†™å›ç´¢å¼•æ–‡ä»¶
            faiss.write_index(self.faiss_index, getattr(self, "faiss_path", os.path.join(os.path.dirname(self.meta_path), "faiss.index")))
            # è¿½åŠ  meta.jsonl ä¸å†…å­˜æ˜ å°?
            with open(self.meta_path, "a", encoding="utf-8") as f:
                for idx, chunk in enumerate(chunks):
                    meta = {"path": path, "chunk_id": idx, "chunk_size": len(chunk)}
                    rec = {"meta": meta, "text": chunk}
                    f.write(json.dumps(rec, ensure_ascii=False) + "\n")
                    self.metas.append(meta)
                    self.texts.append(chunk)
            return len(chunks)

        raise RuntimeError("å½“å‰å‘é‡åç«¯æœªå°±ç»ªï¼Œæ— æ³•æ–°å¢æ–‡æ¡£")

    def delete_document(self, path: str) -> int:
        # Milvus åœ¨çº¿åˆ é™¤
        if self.backend == "milvus" and self.collection is not None:
            escaped = path.replace("'", "\\'")
            expr = "path == '" + escaped + "'"
            res = self.collection.delete(expr)
            self.collection.flush()
            cnt = 0
            try:
                cnt = int(getattr(res, "delete_count", 0))
            except Exception:
                cnt = 0
            return cnt

        # FAISS æœ¬åœ°åˆ é™¤ï¼šè¿‡æ»?meta.jsonlï¼Œå¹¶é‡å»ºç´¢å¼•
        if self.backend == "faiss" and self.faiss_index is not None:
            # è¿‡æ»¤å†…å­˜ä¸­çš„æ–‡æœ¬ä¸å…ƒæ•°æ®
            remain_texts: List[str] = []
            remain_metas: List[Dict[str, Any]] = []
            removed = 0
            for t, m in zip(self.texts, self.metas):
                if str(m.get("path")) == path:
                    removed += 1
                    continue
                remain_texts.append(t)
                remain_metas.append(m)

            # é‡å†™ meta.jsonl
            with open(self.meta_path, "w", encoding="utf-8") as f:
                for m, t in zip(remain_metas, remain_texts):
                    f.write(json.dumps({"meta": m, "text": t}, ensure_ascii=False) + "\n")

            # é‡æ–°ç¼–ç å‰©ä½™æ–‡æœ¬å¹¶é‡å»?faiss ç´¢å¼•
            if remain_texts:
                import faiss  # type: ignore
                vecs = self.embedder.encode(remain_texts, normalize_embeddings=True)
                vecs = np.array(vecs).astype("float32")
                index = faiss.IndexFlatIP(vecs.shape[1])
                index.add(vecs)
                faiss.write_index(index, getattr(self, "faiss_path", os.path.join(os.path.dirname(self.meta_path), "faiss.index")))
                self.faiss_index = index
            else:
                # ç©ºåº“ï¼šé‡å»ºä¸€ä¸ªç©ºç´¢å¼•
                import faiss  # type: ignore
                dim = int(self.embedder.get_sentence_embedding_dimension()) if hasattr(self.embedder, 'get_sentence_embedding_dimension') else len(self.embedder.encode(["dim"], normalize_embeddings=True)[0])
                index = faiss.IndexFlatIP(dim)
                faiss.write_index(index, getattr(self, "faiss_path", os.path.join(os.path.dirname(self.meta_path), "faiss.index")))
                self.faiss_index = index

            # æ›´æ–°å†…å­˜
            self.texts = remain_texts
            self.metas = remain_metas
            return removed

        raise RuntimeError("å½“å‰å‘é‡åç«¯æœªå°±ç»ªï¼Œæ— æ³•åˆ é™¤æ–‡æ¡£")

    def list_paths(self, limit: int = 1000) -> List[str]:
        if self.backend == "milvus" and self.collection is not None:
            try:
                recs = self.collection.query(expr="", output_fields=["path"], limit=limit * 5)
            except Exception:
                recs = []
            paths = []
            seen = set()
            for r in recs:
                p = r.get("path")
                if p and p not in seen:
                    seen.add(p)
                    paths.append(p)
                if len(paths) >= limit:
                    break
            return paths
        # ä»æœ¬åœ?meta å»é‡
        seen = set()
        paths: List[str] = []
        for m in self.metas:
            p = m.get("path")
            if p and p not in seen:
                seen.add(p)
                paths.append(p)
            if len(paths) >= limit:
                break
        return paths
    
    def list_paths_with_stats(self, limit: int = 1000) -> List[dict]:
        """è¿”å›æ–‡æ¡£è·¯å¾„åŠå…¶ç»Ÿè®¡ä¿¡æ¯ï¼ˆåˆ†ç‰‡æ•°ã€æœ€åæ›´æ–°æ—¶é—´ç­‰ï¼?""
        from collections import Counter
        from datetime import datetime
        
        if self.backend == "milvus" and self.collection is not None:
            try:
                recs = self.collection.query(expr="", output_fields=["path", "chunk_id"], limit=limit * 100)
            except Exception:
                recs = []
            
            # ç»Ÿè®¡æ¯ä¸ªè·¯å¾„çš„åˆ†ç‰‡æ•°
            path_chunks = Counter(r.get("path") for r in recs if r.get("path"))
            result = []
            for path, chunk_count in list(path_chunks.items())[:limit]:
                result.append({
                    "path": path,
                    "chunk_count": chunk_count,
                    "last_updated": datetime.now().isoformat()  # Milvus æš‚ä¸æ”¯æŒæ—¶é—´æˆ?
                })
            return result
        
        # FAISS åç«¯ï¼šä»æœ¬åœ° meta ç»Ÿè®¡
        path_chunks = Counter(m.get("path") for m in self.metas if m.get("path"))
        result = []
        for path, chunk_count in list(path_chunks.items())[:limit]:
            result.append({
                "path": path,
                "chunk_count": chunk_count,
                "last_updated": datetime.now().isoformat()
            })
        return result


def build_prompt(question: str, contexts: List[RetrievedChunk], strict_mode: bool = True, custom_system_prompt: str | None = None) -> str:
    """
    æ„å»º RAG æç¤ºè¯?
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µ
        strict_mode: ä¸¥æ ¼æ¨¡å¼ã€‚True=ä»…åŸºäºçŸ¥è¯†åº“å›ç­”ï¼›False=å…è®¸æ¨¡å‹è‡ªç”±å‘æŒ¥
        custom_system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼Œä¼šæ›¿æ¢{context}å’Œ{question}å ä½ç¬?
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡ï¼ˆåˆ†æ•°é˜ˆå€¼æˆ–ä¸ºç©ºï¼?
    has_valid_context = len(contexts) > 0 and any(c.score > 0.1 for c in contexts)
    
    if not has_valid_context and strict_mode:
        # ä¸¥æ ¼æ¨¡å¼ï¼šæ²¡æœ‰å‘½ä¸­çŸ¥è¯†åº“æ—¶ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
        prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„çŸ¥è¯†åº“æ£€ç´¢åŠ©æ‰‹ã€‚\n"
            f"ç”¨æˆ·é—®é¢˜ï¼š{question}\n\n"
            "æ£€ç´¢ç»“æœï¼šæœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚\n\n"
            "è¯·ç¤¼è²Œåœ°å‘Šè¯‰ç”¨æˆ·ï¼š\n"
            "1. å½“å‰çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸è¯¥é—®é¢˜ç›¸å…³çš„èµ„æ–™\n"
            "2. å»ºè®®ç”¨æˆ·è¡¥å……ç›¸å…³æ–‡æ¡£åˆ°çŸ¥è¯†åº“ï¼Œæˆ–æ¢ä¸ªæ–¹å¼æé—®\n"
            "3. ä¸è¦ç¼–é€ æˆ–çŒœæµ‹ç­”æ¡ˆ"
        )
        return prompt
    
    # æœ‰ä¸Šä¸‹æ–‡æˆ–éä¸¥æ ¼æ¨¡å¼ï¼šæ­£å¸¸æ„å»ºæç¤ºè¯
    context_blocks = []
    for i, c in enumerate(contexts, start=1):
        path = c.meta.get("path", "")
        # æå–æ–‡ä»¶åè€Œéå®Œæ•´è·¯å¾„ï¼Œæ›´ç®€æ´?
        filename = path.split('/')[-1] if '/' in path else path.split('\\')[-1] if '\\' in path else path
        score = f"ç›¸å…³åº? {c.score:.2f}"
        context_blocks.append(f"[æ–‡æ¡£{i}: {filename}]\n{c.text}")
    context_text = "\n\n".join(context_blocks)
    
    # å¦‚æœæœ‰è‡ªå®šä¹‰æç¤ºè¯ï¼Œä½¿ç”¨å®ƒå¹¶æ›¿æ¢å ä½ç¬¦ï¼›è‹¥ç¼ºå°‘å ä½ç¬¦åˆ™è‡ªåŠ¨è¡¥å…?
    if custom_system_prompt:
        tpl = custom_system_prompt
        includes_context = "{context}" in tpl
        includes_question = "{question}" in tpl
        prompt = tpl.replace("{context}", context_text).replace("{question}", question)

        # å…œåº•ï¼šç¡®ä¿é—®é¢˜ä¸ä¸Šä¸‹æ–‡ä¸€å®šå‡ºç°åœ¨æœ€ç»ˆæç¤ºä¸­
        if not includes_question or not includes_context:
            prompt = (
                f"{prompt}\n\n"
                f"{'='*60}\n"
                f"ç”¨æˆ·é—®é¢˜ï¼š{question}\n\n"
                f"æ£€ç´¢åˆ°çš„çŸ¥è¯†åº“æ–‡æ¡£ï¼ˆå…± {len(contexts)} ä¸ªç‰‡æ®µï¼‰ï¼š\n\n{context_text}\n"
            )
        return prompt
    
    # å¦åˆ™ä½¿ç”¨é»˜è®¤æç¤ºè¯?
    if strict_mode:
        system_instruction = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“æ£€ç´¢åŠ©æ‰‹ã€‚\n\n"
            "**æ ¸å¿ƒè§„åˆ™**ï¼š\n"
            "1. ä»”ç»†é˜…è¯»ä¸‹åˆ—æ‰€æœ‰æ–‡æ¡£ç‰‡æ®µï¼Œå…¨é¢ç†è§£å…¶å†…å®¹\n"
            "2. ä»æ–‡æ¡£ä¸­å¯»æ‰¾ä¸é—®é¢˜ç›¸å…³çš„**æ‰€æœ‰ä¿¡æ?*ï¼ŒåŒ…æ‹¬ç›´æ¥å’Œé—´æ¥ç›¸å…³çš„å†…å®¹\n"
            "3. ç»¼åˆå¤šä¸ªæ–‡æ¡£ç‰‡æ®µçš„ä¿¡æ¯è¿›è¡Œå›ç­”\n"
            "4. å¦‚æœæ–‡æ¡£ä¸­ç¡®å®æ²¡æœ‰ç­”æ¡ˆï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·\n"
            "5. å›ç­”è¦è¯¦ç»†ã€å…·ä½“ï¼Œå°½å¯èƒ½å¼•ç”¨åŸæ–‡\n\n"
            "**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š\n"
            "1. ä½¿ç”¨è§„èŒƒçš„ä¸­æ–‡æ ‡ç‚¹ç¬¦å·ï¼ˆï¼Œã€‚ï¼›ï¼ï¼Ÿï¼‰\n"
            "2. åˆç†åˆ†æ®µï¼Œæ¯æ®µè®²ä¸€ä¸ªä¸»é¢˜\n"
            "3. ä½¿ç”¨æ ‡é¢˜ã€åˆ—è¡¨ç­‰ Markdown æ ¼å¼æé«˜å¯è¯»æ€§\n"
            "4. æ•°å­—å’Œè‹±æ–‡å‰ååŠ ç©ºæ ¼ï¼ˆä¾‹å¦‚ï¼šYOLOv8 çš„ç»“æ„ï¼‰\n"
            "5. é¿å…å¥å­è¿‡é•¿ï¼Œé€‚å½“æ–­å¥\n\n"
            "**æ³¨æ„**ï¼šå³ä½¿æŸä¸ªæ–‡æ¡£ç‰‡æ®µçœ‹èµ·æ¥ç›¸å…³åº¦ä¸é«˜ï¼Œä¹Ÿè¦ä»”ç»†æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰ç”¨ä¿¡æ¯ã€?
        )
    else:
        system_instruction = (
            "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚\n\n"
            "**ä»»åŠ¡**ï¼š\n"
            "1. ä¼˜å…ˆä½¿ç”¨ä¸‹åˆ—æ–‡æ¡£ç‰‡æ®µä¸­çš„ä¿¡æ¯\n"
            "2. å¦‚æœæ–‡æ¡£ä¿¡æ¯ä¸è¶³ï¼Œå¯ä»¥ç»“åˆä½ çš„çŸ¥è¯†è¿›è¡Œè¡¥å……\n"
            "3. æ˜ç¡®æ ‡æ³¨å“ªäº›æ¥è‡ªæ–‡æ¡£ï¼Œå“ªäº›æ˜¯ä½ çš„è¡¥å……\n"
        )
    
    prompt = (
        f"{system_instruction}\n"
        f"{'='*60}\n"
        f"æ£€ç´¢åˆ°çš„çŸ¥è¯†åº“æ–‡æ¡£ï¼ˆå…± {len(contexts)} ä¸ªç‰‡æ®µï¼‰ï¼š\n\n"
        f"{context_text}\n"
        f"{'='*60}\n\n"
        f"ç”¨æˆ·é—®é¢˜ï¼š{question}\n\n"
        "è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£ï¼Œç»™å‡ºè¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼?
    )
    return prompt


class RAGPipeline:
    """
    RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ? ä¸»æµç¨‹ç®¡é?
    
    æ•´åˆå‘é‡æ£€ç´¢ã€BM25ã€Rerankerã€æŸ¥è¯¢æ”¹å†™å’Œ LLM ç”Ÿæˆï¼Œæä¾›å®Œæ•´çš„é—®ç­”èƒ½åŠ›
    """
    
    def __init__(self, settings: Settings, namespace: str | None = None) -> None:
        """
        åˆå§‹åŒ?RAG Pipeline
        
        Args:
            settings: é…ç½®å¯¹è±¡
            namespace: å‘½åç©ºé—´ï¼Œç”¨äºå¤šç§Ÿæˆ·éš”ç¦»
        """
        self.settings = settings
        meta_path = os.path.join(settings.index_dir, "meta.jsonl")
        self.store = VectorStore(meta_path, settings.embedding_model_name, settings, namespace)

        # é»˜è®¤ä½¿ç”¨ DeepSeek (OpenAI) é…ç½®
        client_kwargs: Dict[str, Any] = {}
        if settings.openai_api_key:
            client_kwargs["api_key"] = settings.openai_api_key
        if settings.openai_base_url:
            client_kwargs["base_url"] = settings.openai_base_url
        self.client = OpenAI(**client_kwargs)
        
        # ä¸?Qwen åˆ›å»ºå•ç‹¬çš„å®¢æˆ·ç«¯
        self.qwen_client = None
        if settings.qwen_api_key:
            qwen_kwargs: Dict[str, Any] = {
                "api_key": settings.qwen_api_key,
                "base_url": settings.qwen_base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"
            }
            self.qwen_client = OpenAI(**qwen_kwargs)
        
        self.reranker = None
        if settings.reranker_enabled and FlagReranker is not None:
            try:
                self.reranker = FlagReranker(settings.reranker_model_name, use_fp16=True)
            except Exception:
                self.reranker = None
        
        # åˆå§‹åŒ–æŸ¥è¯¢æ”¹å†™å™¨ï¼ˆå¯é€‰ï¼‰
        self.query_rewriter = None
        if settings.openai_api_key and settings.openai_base_url:
            try:
                from backend.query_rewriter import QueryRewriter
                self.query_rewriter = QueryRewriter(
                    api_key=settings.openai_api_key,
                    base_url=settings.openai_base_url,
                    model=settings.llm_model
                )
            except Exception as e:
                import logging
                logging.getLogger("rag").warning(f"æŸ¥è¯¢æ”¹å†™å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _get_client_for_model(self, model: str) -> OpenAI:
        """æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©å¯¹åº”çš„å®¢æˆ·ç«¯"""
        if model and model.startswith("qwen"):
            if self.qwen_client is None:
                raise ValueError(f"Qwen æ¨¡å‹ '{model}' éœ€è¦é…ç½?QWEN_API_KEY")
            return self.qwen_client
        return self.client

    def ask(self, question: str, top_k: int | None = None, rerank_enabled: bool | None = None, rerank_top_n: int | None = None, model: str | None = None) -> Tuple[str, List[RetrievedChunk]]:
        k = top_k or self.settings.top_k
        
        # å°è¯•ä»ç¼“å­˜è·å–æ£€ç´¢ç»“æ?
        namespace = getattr(self.store, 'namespace', 'default')
        cached_result = query_cache.get(question, k, namespace)
        if cached_result is not None:
            recs = cached_result
        else:
            recs = self.store.search(question, k)
            # ç¼“å­˜æ£€ç´¢ç»“æ?
            query_cache.set(question, k, namespace, recs)
        # å¯é€‰é‡æ?
        use_rr = (self.reranker is not None) and (self.settings.reranker_enabled if rerank_enabled is None else rerank_enabled)
        top_n = rerank_top_n or self.settings.reranker_top_n
        if use_rr:
            pairs = [[question, r.text] for r in recs]
            scores = self.reranker.compute_score(pairs)
            for r, s in zip(recs, scores):
                r.score = float(s)
            recs = sorted(recs, key=lambda x: x.score, reverse=True)[: top_n]
        prompt = build_prompt(question, recs, strict_mode=self.settings.strict_mode)
        target_model = model or self.settings.llm_model
        client = self._get_client_for_model(target_model)
        resp = client.chat.completions.create(
            model=target_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        answer = resp.choices[0].message.content or ""
        return answer, recs

    def ask_stream(self, question: str, top_k: int | None = None, rerank_enabled: bool | None = None, rerank_top_n: int | None = None, model: str | None = None, system_prompt: str | None = None, web_enabled: bool | None = None, web_top_k: int | None = None):  # noqa: ANN001
        """è¿”å›(ç”Ÿæˆå™? æ£€ç´¢ç‰‡æ®?ã€‚ç”Ÿæˆå™¨é€å—äº§å‡ºæ¨¡å‹æ–‡æœ¬ã€?""
        k = top_k or self.settings.top_k
        recs = self.store.search(question, k)

        # å¯é€‰ï¼šè”ç½‘æœç´¢è¡¥å……å®æ—¶ä¿¡æ¯ï¼ˆç®€å•å®ç°ï¼šè°ƒç”¨ DuckDuckGo html APIï¼?
        web_snippets: list[str] = []
        if web_enabled:
            try:
                import requests
                import re
                n = web_top_k or 3
                q = requests.utils.quote(question)
                url = f"https://duckduckgo.com/html/?q={q}"
                html = requests.get(url, timeout=5).text
                # æç®€æŠ“å–æœç´¢ç»“æœæ‘˜è¦
                results = re.findall(r'<a rel="nofollow" class="result__a" href="(.*?)".*?</a>.*?<a.*?class="result__snippet".*?>(.*?)</a>', html, flags=re.S)
                for link, snippet in results[:n]:
                    text = re.sub('<.*?>', '', snippet)
                    web_snippets.append(f"[Web] {text}\nURL: {link}")
            except Exception:
                pass
        use_rr = (self.reranker is not None) and (self.settings.reranker_enabled if rerank_enabled is None else rerank_enabled)
        top_n = rerank_top_n or self.settings.reranker_top_n
        if use_rr:
            pairs = [[question, r.text] for r in recs]
            scores = self.reranker.compute_score(pairs)
            for r, s in zip(recs, scores):
                r.score = float(s)
            recs = sorted(recs, key=lambda x: x.score, reverse=True)[: top_n]
        # å°?web ç‰‡æ®µæ‹¼æ¥åˆ°ä¸Šä¸‹æ–‡å°¾éƒ¨
        if web_snippets:
            from dataclasses import dataclass
            @dataclass
            class _Tmp:
                text: str
                score: float
                meta: dict
            for w in web_snippets:
                recs.append(_Tmp(text=w, score=1.0, meta={"path": "web"}))

        prompt = build_prompt(question, recs, strict_mode=self.settings.strict_mode, custom_system_prompt=system_prompt)
        target_model = model or self.settings.llm_model
        client = self._get_client_for_model(target_model)

        def _gen():  # noqa: ANN202
            stream = client.chat.completions.create(
                model=target_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                stream=True,
            )
            for chunk in stream:
                delta = getattr(getattr(chunk.choices[0], "delta", None), "content", None)
                if delta:
                    yield delta

        return _gen(), recs
    
    def ask_with_query_rewriting(
        self, 
        question: str, 
        strategy: str = "expand",
        top_k: int | None = None,
        rerank_enabled: bool | None = None,
        rerank_top_n: int | None = None,
        model: str | None = None
    ) -> Tuple[str, List[RetrievedChunk], Dict[str, Any]]:
        """
        ä½¿ç”¨æŸ¥è¯¢æ”¹å†™å¢å¼ºæ£€ç´¢æ•ˆæ?
        
        Args:
            question: åŸå§‹æŸ¥è¯¢
            strategy: æ”¹å†™ç­–ç•¥ (expand/decompose/hyde/multi)
            top_k: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„æ–‡æ¡£æ•°
            rerank_enabled: æ˜¯å¦å¯ç”¨é‡æ’
            rerank_top_n: é‡æ’åä¿ç•™çš„æ–‡æ¡£æ•?
            model: LLM æ¨¡å‹
        
        Returns:
            (ç­”æ¡ˆ, æ£€ç´¢ç‰‡æ®µåˆ—è¡? å…ƒæ•°æ?
            å…ƒæ•°æ®åŒ…å? original_query, rewritten_queries, strategy
        """
        if not self.query_rewriter:
            # æŸ¥è¯¢æ”¹å†™å™¨æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°æ™®é€šæ£€ç´?
            answer, recs = self.ask(question, top_k, rerank_enabled, rerank_top_n, model)
            return answer, recs, {
                "original_query": question,
                "rewritten_queries": [question],
                "strategy": "none",
                "note": "æŸ¥è¯¢æ”¹å†™å™¨æœªå¯ç”¨"
            }
        
        # 1. æ”¹å†™æŸ¥è¯¢
        rewritten_queries = self.query_rewriter.rewrite_for_retrieval(question, strategy)
        
        # 2. å¯¹æ¯ä¸ªæ”¹å†™åçš„æŸ¥è¯¢è¿›è¡Œæ£€ç´?
        k = top_k or self.settings.top_k
        all_recs: List[RetrievedChunk] = []
        seen_texts = set()
        
        for query in rewritten_queries:
            recs = self.store.search(query, k)
            # å»é‡ï¼šé¿å…ç›¸åŒæ–‡æ¡£ç‰‡æ®µé‡å¤å‡ºç?
            for rec in recs:
                if rec.text not in seen_texts:
                    seen_texts.add(rec.text)
                    all_recs.append(rec)
        
        # 3. å¯¹åˆå¹¶åçš„ç»“æœè¿›è¡Œé‡æ’ï¼ˆå¯é€‰ï¼‰
        use_rr = (self.reranker is not None) and (self.settings.reranker_enabled if rerank_enabled is None else rerank_enabled)
        top_n = rerank_top_n or self.settings.reranker_top_n
        
        if use_rr and all_recs:
            # ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¿›è¡Œé‡æ’
            pairs = [[question, r.text] for r in all_recs]
            scores = self.reranker.compute_score(pairs)
            for r, s in zip(all_recs, scores):
                r.score = float(s)
            all_recs = sorted(all_recs, key=lambda x: x.score, reverse=True)[:top_n]
        else:
            # æŒ‰åŸå§‹åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            all_recs = sorted(all_recs, key=lambda x: x.score, reverse=True)[:top_n]
        
        # 4. ç”Ÿæˆç­”æ¡ˆ
        prompt = build_prompt(question, all_recs, strict_mode=self.settings.strict_mode)
        target_model = model or self.settings.llm_model
        client = self._get_client_for_model(target_model)
        resp = client.chat.completions.create(
            model=target_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        answer = resp.choices[0].message.content or ""
        
        # 5. è¿”å›ç»“æœå’Œå…ƒæ•°æ®
        metadata = {
            "original_query": question,
            "rewritten_queries": rewritten_queries,
            "strategy": strategy,
            "total_retrieved": len(all_recs),
            "unique_documents": len(seen_texts)
        }
        
        return answer, all_recs, metadata
    
    def analyze_query(self, question: str) -> Dict[str, Any]:
        """
        åˆ†ææŸ¥è¯¢ç‰¹å¾å¹¶æ¨èæœ€ä½³æ”¹å†™ç­–ç•?
        
        Args:
            question: ç”¨æˆ·æŸ¥è¯¢
        
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        if not self.query_rewriter:
            return {
                "error": "æŸ¥è¯¢æ”¹å†™å™¨æœªå¯ç”¨",
                "recommended_strategy": "none"
            }
        
        return self.query_rewriter.analyze_query(question)

    def add_document(self, path: str, text: str) -> int:
        return self.store.add_document(path, text)

    def delete_document(self, path: str) -> int:
        return self.store.delete_document(path)

    def list_paths(self, limit: int = 1000) -> List[str]:
        return self.store.list_paths(limit)
    
    def list_paths_with_stats(self, limit: int = 1000) -> List[dict]:
        return self.store.list_paths_with_stats(limit)



def _dedupe_results(recs: List[RetrievedChunk]) -> List[RetrievedChunk]:
    seen = set()
    out: List[RetrievedChunk] = []
    for r in recs:
        key = (r.meta.get("path"), r.meta.get("chunk_id"), r.text[:200])
        if key in seen:
            continue
        seen.add(key)
        out.append(r)
    return out

def _apply_score_threshold(recs: List[RetrievedChunk], threshold: float) -> List[RetrievedChunk]:
    if threshold <= 0:
        return recs
    return [r for r in recs if r.score >= threshold]

