"""
æ£€ç´¢ç»“æœè§£é‡Šæ¨¡å—

æä¾›æ£€ç´¢ç»“æœçš„è¯„åˆ†è§£é‡Šå’Œå¯è§†åŒ–ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£ä¸ºä»€ä¹ˆæ£€ç´¢åˆ°è¿™äº›æ–‡æ¡£
"""

import logging
from typing import List, Dict, Any
from dataclasses import dataclass
import re

logger = logging.getLogger("rag")


@dataclass
class ScoreBreakdown:
    """è¯„åˆ†ç»†åˆ†"""
    vector_score: float = 0.0  # å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°
    bm25_score: float = 0.0    # BM25 åˆ†æ•°
    rerank_score: float = 0.0  # é‡æ’åˆ†æ•°
    final_score: float = 0.0   # æœ€ç»ˆåˆ†æ•°
    
    # è§£é‡Šä¿¡æ¯
    matched_keywords: List[str] = None  # åŒ¹é…çš„å…³é”®è¯
    semantic_similarity: str = ""        # è¯­ä¹‰ç›¸ä¼¼åº¦æè¿°
    explanation: str = ""                # æ€»ä½“è§£é‡Š


@dataclass
class RetrievalExplanation:
    """æ£€ç´¢ç»“æœè§£é‡Š"""
    chunk_id: int
    text: str
    score_breakdown: ScoreBreakdown
    relevance_level: str  # high/medium/low
    highlight_text: str   # é«˜äº®æ˜¾ç¤ºçš„æ–‡æœ¬
    metadata: Dict[str, Any]


class RetrievalExplainer:
    """æ£€ç´¢ç»“æœè§£é‡Šå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è§£é‡Šå™¨"""
        logger.info("RetrievalExplainer åˆå§‹åŒ–å®Œæˆ")
    
    def extract_keywords(self, query: str) -> List[str]:
        """
        ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
        
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        query_clean = re.sub(r'[^\w\s]', ' ', query)
        
        # åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼åˆ†ï¼‰
        words = query_clean.split()
        
        # è¿‡æ»¤åœç”¨è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ç­‰', 'åŠ', 
                     'ä¸º', 'ä»¥', 'åˆ°', 'å¯¹', 'ä»', 'è€Œ', 'ä½†', 'ä¹Ÿ', 'éƒ½', 'å°±',
                     'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}
        
        keywords = [w for w in words if w and w.lower() not in stopwords and len(w) > 1]
        
        return keywords
    
    def find_matched_keywords(self, query: str, text: str) -> List[str]:
        """
        æ‰¾å‡ºæŸ¥è¯¢ä¸­åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„å…³é”®è¯
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            text: æ–‡æ¡£æ–‡æœ¬
        
        Returns:
            åŒ¹é…çš„å…³é”®è¯åˆ—è¡¨
        """
        keywords = self.extract_keywords(query)
        text_lower = text.lower()
        
        matched = []
        for kw in keywords:
            if kw.lower() in text_lower:
                matched.append(kw)
        
        return matched
    
    def highlight_text(self, text: str, keywords: List[str], max_length: int = 300) -> str:
        """
        é«˜äº®æ˜¾ç¤ºæ–‡æœ¬ä¸­çš„å…³é”®è¯
        
        Args:
            text: åŸæ–‡æœ¬
            keywords: è¦é«˜äº®çš„å…³é”®è¯
            max_length: æœ€å¤§æ˜¾ç¤ºé•¿åº¦
        
        Returns:
            é«˜äº®åçš„æ–‡æœ¬ï¼ˆä½¿ç”¨ **keyword** æ ‡è®°ï¼‰
        """
        if not keywords:
            return text[:max_length] + ("..." if len(text) > max_length else "")
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå…³é”®è¯çš„ä½ç½®
        first_match_pos = len(text)
        for kw in keywords:
            pos = text.lower().find(kw.lower())
            if pos != -1 and pos < first_match_pos:
                first_match_pos = pos
        
        # ä»¥ç¬¬ä¸€ä¸ªå…³é”®è¯ä¸ºä¸­å¿ƒæˆªå–æ–‡æœ¬
        if first_match_pos < len(text):
            start = max(0, first_match_pos - 100)
            end = min(len(text), first_match_pos + 200)
            snippet = text[start:end]
            if start > 0:
                snippet = "..." + snippet
            if end < len(text):
                snippet = snippet + "..."
        else:
            snippet = text[:max_length]
        
        # é«˜äº®å…³é”®è¯
        for kw in keywords:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„æ›¿æ¢
            pattern = re.compile(re.escape(kw), re.IGNORECASE)
            snippet = pattern.sub(f"**{kw}**", snippet)
        
        return snippet
    
    def get_relevance_level(self, score: float) -> str:
        """
        æ ¹æ®åˆ†æ•°åˆ¤æ–­ç›¸å…³æ€§ç­‰çº§
        
        Args:
            score: ç›¸å…³æ€§åˆ†æ•°
        
        Returns:
            ç›¸å…³æ€§ç­‰çº§ (high/medium/low)
        """
        if score >= 0.75:
            return "high"
        elif score >= 0.5:
            return "medium"
        else:
            return "low"
    
    def get_semantic_similarity_desc(self, score: float) -> str:
        """
        è·å–è¯­ä¹‰ç›¸ä¼¼åº¦æè¿°
        
        Args:
            score: ç›¸ä¼¼åº¦åˆ†æ•°
        
        Returns:
            æè¿°æ–‡æœ¬
        """
        if score >= 0.9:
            return "æé«˜ç›¸ä¼¼åº¦"
        elif score >= 0.75:
            return "é«˜åº¦ç›¸ä¼¼"
        elif score >= 0.6:
            return "ä¸­ç­‰ç›¸ä¼¼"
        elif score >= 0.4:
            return "ä½åº¦ç›¸ä¼¼"
        else:
            return "å¼±ç›¸å…³"
    
    def explain_retrieval(
        self,
        query: str,
        retrieved_chunks: List[Dict[str, Any]],
        include_scores: bool = True
    ) -> List[RetrievalExplanation]:
        """
        è§£é‡Šæ£€ç´¢ç»“æœ
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_chunks: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
                æ¯ä¸ªç‰‡æ®µåŒ…å«: text, score, meta
            include_scores: æ˜¯å¦åŒ…å«è¯¦ç»†è¯„åˆ†
        
        Returns:
            è§£é‡Šç»“æœåˆ—è¡¨
        """
        explanations = []
        
        for i, chunk in enumerate(retrieved_chunks):
            text = chunk.get("text", "")
            score = chunk.get("score", 0.0)
            meta = chunk.get("meta", {})
            
            # æå–åŒ¹é…çš„å…³é”®è¯
            matched_keywords = self.find_matched_keywords(query, text)
            
            # ç”Ÿæˆé«˜äº®æ–‡æœ¬
            highlight_text = self.highlight_text(text, matched_keywords)
            
            # è¯„åˆ†ç»†åˆ†
            score_breakdown = ScoreBreakdown(
                vector_score=score,  # ç®€åŒ–ï¼šè¿™é‡Œåªæœ‰æœ€ç»ˆåˆ†æ•°
                final_score=score,
                matched_keywords=matched_keywords,
                semantic_similarity=self.get_semantic_similarity_desc(score)
            )
            
            # ç”Ÿæˆè§£é‡Š
            explanation_parts = []
            
            if matched_keywords:
                explanation_parts.append(
                    f"åŒ¹é…äº† {len(matched_keywords)} ä¸ªå…³é”®è¯: {', '.join(matched_keywords[:5])}"
                )
            
            explanation_parts.append(
                f"è¯­ä¹‰ç›¸ä¼¼åº¦: {score_breakdown.semantic_similarity} ({score:.2f})"
            )
            
            if meta.get("page"):
                explanation_parts.append(f"æ¥è‡ªç¬¬ {meta['page']} é¡µ")
            
            if meta.get("has_tables"):
                explanation_parts.append("åŒ…å«è¡¨æ ¼æ•°æ®")
            
            score_breakdown.explanation = " | ".join(explanation_parts)
            
            # åˆ¤æ–­ç›¸å…³æ€§ç­‰çº§
            relevance_level = self.get_relevance_level(score)
            
            # åˆ›å»ºè§£é‡Šå¯¹è±¡
            explanation = RetrievalExplanation(
                chunk_id=i,
                text=text,
                score_breakdown=score_breakdown,
                relevance_level=relevance_level,
                highlight_text=highlight_text,
                metadata=meta
            )
            
            explanations.append(explanation)
        
        logger.info(f"ç”Ÿæˆäº† {len(explanations)} ä¸ªæ£€ç´¢ç»“æœè§£é‡Š")
        return explanations
    
    def generate_summary(self, explanations: List[RetrievalExplanation]) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ£€ç´¢ç»“æœæ‘˜è¦
        
        Args:
            explanations: è§£é‡Šåˆ—è¡¨
        
        Returns:
            æ‘˜è¦ä¿¡æ¯
        """
        if not explanations:
            return {
                "total_chunks": 0,
                "relevance_distribution": {},
                "avg_score": 0.0,
                "top_keywords": []
            }
        
        # ç»Ÿè®¡ç›¸å…³æ€§åˆ†å¸ƒ
        relevance_dist = {"high": 0, "medium": 0, "low": 0}
        for exp in explanations:
            relevance_dist[exp.relevance_level] += 1
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_score = sum(exp.score_breakdown.final_score for exp in explanations) / len(explanations)
        
        # ç»Ÿè®¡æœ€å¸¸è§çš„å…³é”®è¯
        all_keywords = []
        for exp in explanations:
            if exp.score_breakdown.matched_keywords:
                all_keywords.extend(exp.score_breakdown.matched_keywords)
        
        from collections import Counter
        keyword_counts = Counter(all_keywords)
        top_keywords = [kw for kw, _ in keyword_counts.most_common(10)]
        
        return {
            "total_chunks": len(explanations),
            "relevance_distribution": relevance_dist,
            "avg_score": round(avg_score, 3),
            "top_keywords": top_keywords,
            "high_relevance_count": relevance_dist["high"],
            "medium_relevance_count": relevance_dist["medium"],
            "low_relevance_count": relevance_dist["low"]
        }
    
    def format_explanation_text(self, explanation: RetrievalExplanation) -> str:
        """
        æ ¼å¼åŒ–è§£é‡Šä¸ºå¯è¯»æ–‡æœ¬
        
        Args:
            explanation: è§£é‡Šå¯¹è±¡
        
        Returns:
            æ ¼å¼åŒ–çš„æ–‡æœ¬
        """
        lines = []
        lines.append(f"ğŸ“„ æ–‡æ¡£ç‰‡æ®µ #{explanation.chunk_id + 1}")
        lines.append(f"ğŸ“Š ç›¸å…³æ€§: {explanation.relevance_level.upper()} ({explanation.score_breakdown.final_score:.2f})")
        lines.append(f"ğŸ’¡ {explanation.score_breakdown.explanation}")
        lines.append(f"\nğŸ“ å†…å®¹é¢„è§ˆ:\n{explanation.highlight_text}")
        
        return "\n".join(lines)


def create_explainer() -> RetrievalExplainer:
    """åˆ›å»ºæ£€ç´¢è§£é‡Šå™¨å®ä¾‹"""
    return RetrievalExplainer()
